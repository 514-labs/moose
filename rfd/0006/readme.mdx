---
authors: Nicolas Joseph <nicolas@fiveonefour.com>
title: Moose SQL Dependency Management for ClickHouse
state: prediscussion
---

## Abstract

This RFD proposes a solution to reliably manage Data Definition Language (DDL) changes for dependent SQL objects within Moose,
specifically focusing on ClickHouse Materialized Views (MVs), Views, and Tables during development (`moose dev`). The current approach
executes changes independently, leading to potential DDL errors when dependent objects are modified concurrently. The proposed solution mandates
explicit dependency tracking in resource definitions and introduces an intermediate step between infrastructure planning (diffing) and execution.
This step will analyze dependencies using a graph, decompose changes into atomic operations, and generate an ordered, phased execution plan (Teardown,
Setup/Modify) based on topological sorting, similar to migration tool principles.

## Motivation / Problem Description

Moose provides a declarative way for users to define infrastructure, including ClickHouse tables, materialized views, and other SQL resources.
During development (`moose dev`), the framework compares the user's desired state with the current state, generates a plan of changes
(Adds, Removes, Updates), and executes these changes against the ClickHouse database.

Currently, each infrastructure component change (`TableChange`, `SqlResource` change, etc.) is calculated and executed independently.
This poses a significant problem when dealing with SQL objects that have inherent dependencies, most notably:

1.  **Materialized Views (MVs):** An MV reads from one or more source tables/views (potentially via JOINs) and writes to a target table.
    Its creation/update depends on all sources and the target existing and being correct.
2.  **Chained MVs/Views:** MV2/View2 might read from the target table of MV1 or another View1.
3.  **Views on Tables/Views:** A standard View depends on the objects in its `SELECT` statement.

If a user modifies both a dependent object (e.g., an MV's `SELECT` query) and any of its dependencies (e.g., a source table schema, the target
table schema) in the same development iteration, the current system generates separate, unordered change operations. The execution order of
these independent changes is not guaranteed to respect the database constraints.

This can lead to errors during `moose dev` runs, such as:
*   `CREATE MATERIALIZED VIEW` failing because a source or target table schema doesn't match or hasn't been altered yet.
*   `CREATE VIEW` failing because a source table it selects from hasn't been created/altered yet.
*   `ALTER TABLE` failing because a dependent View/MV still exists and locks the table schema.

These failures result in an inconsistent infrastructure state, break the development feedback loop, and force users to manually intervene or rely
on subsequent `moose dev` runs to potentially fix the state.

Furthermore, the current update mechanism for MVs/Views (represented as `SqlResource`) involves dropping and recreating them, which can be
inefficient. While this RFD doesn't prioritize optimizing the update itself (e.g., using `ALTER VIEW`), the proposed solution provides a
better foundation for future improvements by correctly ordering even drop/recreate steps.

## Goals

*   Ensure the correct order of DDL execution for dependent ClickHouse objects managed by Moose.
*   Improve the reliability and predictability of infrastructure updates during `moose dev`.
*   Prevent common DDL errors caused by incorrect change application order.
*   Maintain the declarative nature of Moose infrastructure definitions for the user.
*   Establish a robust pattern for handling SQL dependencies.

## Non-Goals

*   Implementing full DDL transactionality for ClickHouse changes (a ClickHouse limitation).
*   Optimizing the update mechanism for MVs/Views beyond drop-and-recreate within *this* RFD.
*   Managing dependencies for resources *not* managed by Moose within the ClickHouse database.
*   Parsing SQL `SELECT` statements to automatically infer dependencies (we rely on explicit declaration).

## Proposed Solution: Explicit Dependencies & Phased Ordering

We propose a two-part solution: mandating explicit dependency tracking in the APIs and introducing an intermediate step for dependency analysis
and phased, ordered execution planning.

### Part 1: Explicit Dependency Tracking via APIs (Mandatory)

Robust dependency management requires clear dependency information, which is derived by the Moose framework.

1.  **API/SDK Definition & Responsibility:**
    *   Users define resources like `MaterializedView`, `View`, etc., using the Moose SDKs (e.g., `ts-moose-lib`, `py-moose-lib`).
    *   Dependencies are often **implicitly derived** by the SDKs from resource-specific configurations (e.g., the source/target tables
        referenced in a `MaterializedView` definition) rather than requiring a universal, explicit `depends_on` array provided by the user
        in all cases.
    *   **SDK Responsibility:** The Moose SDK layer (during the code execution phase that generates the configuration for the Rust CLI)
        analyzes the resource definitions and determines the data lineage (which resources are read from or written to). It translates this
        lineage information into a structured Protobuf-based format.
    *   Example (Conceptual TS - Materialized View):
        ```typescript
        import { sourceTable1, targetTable } from './resources';
        // Assume sourceTable1, targetTable are instances of OlapTable...

        new MaterializedView<typeof targetTable.schema>(
          'my_mv',
          {
              selectStatement: sql`SELECT col1 FROM ${sourceTable1.tableName}`,
              selectTables: [
                  sourceTable1,
              ],
          }
        );
        ```
        *(Internal SDK transformation generates Protobuf messages representing these dependencies.)*
2.  **Protobuf Representation (`infrastructure_map.proto`):**
    *   A dedicated `InfrastructureSignature` Protobuf message is defined to uniquely identify any Moose-managed resource (Table, Topic,
        View, SqlResource, etc.) using its type and ID.
    *   This `InfrastructureSignature` is used within other Protobuf messages to represent dependencies, often via fields like
        `pulls_data_from` and `pushes_data_to` rather than a single generic `depends_on`.
3.  **Rust Infrastructure Representation (`infrastructure_map.rs`, DataLineage trait):**
    *   Rust structs representing resources (e.g., `SqlResource`, `View`, `Table`, various processes) implement a `DataLineage` trait.
    *   This trait provides methods like `pulls_data_from(&self) -> Vec<ProtoInfrastructureSignature>` and
        `pushes_data_to(&self) -> Vec<ProtoInfrastructureSignature>`.
    *   These methods return vectors of the Protobuf-derived `ProtoInfrastructureSignature` struct, indicating the direct upstream sources
        and downstream targets for that resource.
    *   The `diff` and planning logic uses this `DataLineage` trait to query the dependency graph dynamically.
4.  **Benefit:** The dependency graph is implicitly defined through resource configurations and explicitly accessible in Rust via the
    standardized `DataLineage` trait using the canonical `ProtoInfrastructureSignature` type. The analysis step can reliably query this
    information.

### Part 2: Intermediate Dependency Analysis & Phased Ordering Step

This step generates an ordered execution plan based on the explicit dependencies.

1.  **Position:** `diff -> [Dependency Analysis & Phased Ordering] -> execute_changes`
2.  **Input:**
    *   The "flat" list of `OlapChange`s generated by `diff`.
    *   The target `InfrastructureMap` (containing resources with populated `depends_on` fields).
3.  **Process:**
    *   **Build Dependency Graph:** Construct a Directed Acyclic Graph (DAG) of all relevant resources (Tables, SqlResources representing MVs, Views)
        using the `depends_on` relationships. Nodes are resource IDs, edges represent dependencies.
    *   **Deconstruct Changes:** Break down each `OlapChange` (e.g., `Table(Added(t))`, `SqlResource(Updated{before, after})`) into its atomic DDL
        operation(s) (e.g., `CREATE TABLE...`, `DROP VIEW...`, `ALTER TABLE...`, `CREATE VIEW...`). Tag each atomic operation as either "Teardown" or
        "Setup/Modify" and associate it with the resource ID it affects.
    *   **Phase 1: Generate Teardown Sequence:**
        *   Collect all atomic "Teardown" operations.
        *   Perform a **reverse topological sort** on the dependency graph for the involved resource IDs.
        *   Output: An ordered list (`teardown_plan`) of atomic DDL operations (dependents are torn down before dependencies).
    *   **Phase 2: Generate Setup/Modification Sequence:**
        *   Collect all atomic "Setup/Modify" operations.
        *   Perform a **forward topological sort** on the dependency graph for the involved resource IDs.
        *   Output: An ordered list (`setup_modify_plan`) of atomic DDL operations (dependencies are created/altered before dependents).
    *   _Justification for Phased Plans:_ Separating teardown from setup/modification ensures that dependent objects are removed before their
        dependencies are altered or dropped, and that dependencies exist before dependents are created or updated. This avoids DDL conflicts
        and simplifies the ordering logic compared to interleaving all operations.
4.  **Output:** Two ordered lists of atomic DDL operations: `teardown_plan` and `setup_modify_plan`.

### Part 3: Simplified Execution Phase (`clickhouse.rs`)

The `execute_changes` function becomes a straightforward executor.

1.  **Input:** The ordered `teardown_plan` and `setup_modify_plan` lists.
2.  **Process:**
    *   Execute all operations in `teardown_plan` sequentially. Stop and report on first error.
    *   If teardown succeeded, execute all operations in `setup_modify_plan` sequentially. Stop and report on first error.
3.  **Benefit:** Execution logic is simple. All complexity related to dependencies and ordering resides in the intermediate step.

### Part 4: State Reconciliation and Edge Cases

A critical aspect of this design is handling situations where the actual state of the ClickHouse database diverges from the last known successfully
applied state (represented by the saved `InfrastructureMap`, e.g., in `.moose/state.json`). This divergence can happen due to failed previous
`moose dev` runs or external manual changes.

The proposed solution relies on **determining the actual state of managed resources in the database *before* performing the `diff` operation** on
each `moose dev` run (as discussed in Open Questions - State Reconciliation Strategy). This ensures the generated plan correctly bridges the gap
from *reality* to the *target* state.

Let's examine how this handles specific edge cases:

**Scenario 1: Resource Updated Externally (e.g., Manual `ALTER TABLE`)**

*   **Situation:**
    *   Saved State (`state.json`): Defines `Table T1 { Column A }`.
    *   Actual DB State: Someone manually ran `ALTER TABLE T1 ADD COLUMN B`. DB has `Table T1 { Column A, Column B }`.
    *   Target State (User Code): Still defines `Table T1 { Column A }`.
*   **Handling (`moose dev` run):**
    1.  **Load Saved State:** Framework notes last success was `Table T1 { Column A }`.
    2.  **Determine Actual State:** Framework queries ClickHouse (`system.tables`, `system.columns`) and finds `Table T1 { Column A, Column B }`.
    3.  **Determine Target State:** Framework runs user code and gets target `Table T1 { Column A }`.
    4.  **`diff(actual, target)`:** Compares state from Step 2 and Step 3. Detects that `Column B` exists in `actual` but not in `target`.
    5.  **Plan Generation:** The intermediate step generates plans:
        *   `teardown_plan`: May include steps to drop dependents of `T1` if the column removal requires it (based on policy).
        *   `setup_modify_plan`: Will include the atomic DDL `ALTER TABLE T1 DROP COLUMN B`.
    6.  **Execution:** The plans are executed, bringing the database back in line with the declarative code definition.
    7.  **Save State:** On success, the framework saves the target state (`Table T1 { Column A }`).

**Scenario 2: Resource Partially Updated by Failed Run**

*   **Situation:**
    *   Saved State: Defines `Table T1 { Column A }`.
    *   Previous Target State: Defined `Table T1 { Column A, Column B }`.
    *   Previous Run Failure: Teardown succeeded. Setup/Modify ran `ALTER TABLE T1 ADD COLUMN B` successfully, but then failed on a *subsequent*
        step. State was *not* saved.
    *   Actual DB State: `Table T1 { Column A, Column B }`.
    *   Current Target State (User Code): User hasn't changed code, still targets `Table T1 { Column A, Column B }`.
*   **Handling (`moose dev` run):**
    1.  **Load Saved State:** Framework notes last success was `Table T1 { Column A }`.
    2.  **Determine Actual State:** Framework queries ClickHouse and finds `Table T1 { Column A, Column B }`.
    3.  **Determine Target State:** Framework runs user code and gets target `Table T1 { Column A, Column B }`.
    4.  **`diff(actual, target)`:** Compares state from Step 2 and Step 3. Finds no difference for `Table T1` itself. Differences might exist for
        *other* resources that failed to update in the previous run.
    5.  **Plan Generation:** The plan will focus only on the remaining differences (e.g., creating/altering other objects from the previous failed
        plan). It will *not* contain operations for `Table T1`.
    6.  **Execution:** The corrective plan is executed.
    7.  **Save State:** On success, the framework saves the target state (`Table T1 { Column A, Column B }`).

**Scenario 3: Resource Removed Externally (e.g., Manual `DROP TABLE`)**

*   **Situation:**
    *   Saved State: Defines `Table T1`.
    *   Actual DB State: Someone manually ran `DROP TABLE T1`. `T1` does not exist.
    *   Target State (User Code): Still defines `Table T1`.
*   **Handling (`moose dev` run):**
    1.  **Load Saved State:** Framework notes last success included `Table T1`.
    2.  **Determine Actual State:** Framework queries ClickHouse and finds `Table T1` is missing.
    3.  **Determine Target State:** Framework runs user code and gets target defining `Table T1`.
    4.  **`diff(actual, target)`:** Compares state from Step 2 and Step 3. Detects `Table T1` as `Added` (present in target, absent in actual).
    5.  **Plan Generation:**
        *   `teardown_plan`: Empty regarding `T1`.
        *   `setup_modify_plan`: Will include the atomic DDL `CREATE TABLE T1 ...`.
    6.  **Execution:** The plans are executed, recreating `T1`.
    7.  **Save State:** On success, the framework saves the target state including `Table T1`.

**Scenario 4: Resource Removed by Failed Run (Successful Teardown)**

*   **Situation:**
    *   Saved State: Defines `Table T1`.
    *   Previous Target State: Did *not* define `Table T1`.
    *   Previous Run Failure: Teardown phase successfully ran `DROP TABLE T1`. Setup/Modify phase failed later. State was *not* saved.
    *   Actual DB State: `Table T1` does not exist.
    *   Current Target State (User Code): User hasn't changed code, still does *not* define `Table T1`.
*   **Handling (`moose dev` run):**
    1.  **Load Saved State:** Framework notes last success included `Table T1`.
    2.  **Determine Actual State:** Framework queries ClickHouse and finds `Table T1` is missing.
    3.  **Determine Target State:** Framework runs user code and gets target *without* `Table T1`.
    4.  **`diff(actual, target)`:** Compares state from Step 2 and Step 3. Finds no difference regarding `Table T1` (it's absent in both).
    5.  **Plan Generation:** The plan will *not* contain any operations for `Table T1`. It will focus only on other differences caused by the previous failure.
    6.  **Execution:** The corrective plan is executed.
    7.  **Save State:** On success, the framework saves the target state *without* `Table T1`.

**Conclusion for Part 4:**

By incorporating the step of determining the actual database state before diffing, the proposed phased execution model becomes resilient to inconsistencies
caused by prior failures or external changes. It ensures that the generated plan always aims to transition the database from its *current reality*
to the user's *intended target state*, rather than relying solely on the potentially stale saved state.

## Alternative Solutions Considered

1.  **Bundling Approach:** (Previously discussed) Create composite change types. Deemed insufficient for multi-source dependencies.
2.  **Modify Diff Algorithm:** Integrate dependency analysis and ordering directly into `diff`. Makes `diff` significantly more complex.
3.  **Manual Ordering / Migrations:** Require users to write ordered migration scripts. Sacrifices the declarative approach.

## Competitor Analysis / Prior Art

*   **dbt (Data Build Tool):** Uses `ref()` for automatic DAG building and topological execution. Strongest parallel.
*   **Terraform/Pulumi:** Declarative IaC, handles resource dependencies but less SQL-aware.
*   **Flyway/Liquibase:** Versioned migrations, relies on manual ordering. The proposed Moose solution aims for dbt-like automatic ordering derived
    from declarative, explicitly dependent definitions.

## Open Questions / Future Considerations

*   Confirm the exact policy for recreating downstream objects when an upstream dependency changes but the downstream object's definition does not.
    (Default: Recreate for safety).
*   Explore future SQL parsing (`sqlparser-rs`) to *validate* user-provided `depends_on` lists against the `SELECT` statement, or potentially
    infer them if explicit declaration becomes burdensome.
*   Performance of graph building and topological sort for large schemas.