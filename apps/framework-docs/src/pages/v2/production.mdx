# Production Guide

Learn how to deploy, monitor, and operate DMv2 pipelines in production.

## Monitoring

DMv2 provides built-in metrics for monitoring your data pipelines:

```typescript
// Available metrics
interface DMv2Metrics {
  // Ingestion metrics
  "ingestion.records_total": Counter;          // Total records received
  "ingestion.errors_total": Counter;           // Failed ingestion attempts
  "ingestion.validation_errors": Counter;      // Schema validation errors
  "ingestion.batch_size": Histogram;          // Batch size distribution
  "ingestion.latency_ms": Histogram;          // Ingestion latency

  // Stream metrics
  "stream.records_processed": Counter;         // Processed records
  "stream.processing_errors": Counter;         // Processing failures
  "stream.retry_attempts": Counter;           // Retry attempts
  "stream.processing_lag": Gauge;            // Processing delay
  "stream.batch_processing_duration": Histogram; // Batch processing time

  // Storage metrics
  "storage.records_written": Counter;         // Records written to storage
  "storage.write_errors": Counter;           // Failed write attempts
  "storage.write_latency_ms": Histogram;     // Write latency
  "storage.storage_size_bytes": Gauge;       // Storage size
}
```

### Monitoring Configuration

```typescript
import { IngestPipeline, Key } from '@514labs/moose-lib';

interface UserEvent {
  id: Key<string>;
  userId: string;
  action: string;
  timestamp: Date;
}

// Enable detailed monitoring
const userEvents = new IngestPipeline<UserEvent>("user_events", {
  monitoring: {
    // Enable all metrics
    metrics: true,
    
    // Custom metric labels
    labels: {
      environment: process.env.NODE_ENV,
      service: "user-events"
    },
    
    // Metric export configuration
    exporters: {
      prometheus: true,     // Export to Prometheus
      statsd: {            // Export to StatsD
        host: "localhost",
        port: 8125,
        prefix: "dmv2"
      }
    }
  }
});
```

## Recovery and Replay

DMv2 supports point-in-time recovery and data replay:

```typescript
import { ReplayManager } from '@514labs/moose-lib';

// Configure replay options
const replayManager = new ReplayManager({
  // Retention configuration
  retention: {
    duration: "30d",      // Keep 30 days of data
    snapshots: true       // Enable point-in-time snapshots
  },
  
  // Storage configuration
  storage: {
    type: "s3",
    bucket: "replay-data",
    prefix: "events/"
  }
});

// Replay data from a specific timestamp
await replayManager.replayFrom({
  pipeline: "user_events",
  startTime: new Date("2024-03-20T00:00:00Z"),
  endTime: new Date("2024-03-21T00:00:00Z")
});

// Replay specific records
await replayManager.replayRecords({
  pipeline: "user_events",
  recordIds: ["evt_123", "evt_124"]
});
```

## Troubleshooting

### High Processing Lag

```typescript
// Monitor processing lag
const monitor = new StreamMonitor("user_events");

monitor.on("highLag", async ({ lag, threshold }) => {
  // Alert on high lag
  if (lag > threshold) {
    await alerting.send({
      title: "High Processing Lag",
      message: `Processing lag of ${lag}ms exceeds threshold of ${threshold}ms`,
      severity: "warning"
    });
  }
});

// Adjust processing parameters
const userEvents = new IngestPipeline<UserEvent>("user_events", {
  stream: {
    parallelism: 4,        // Increase parallel processing
    batchSize: 100,        // Process more records per batch
    maxRetries: 3          // Limit retry attempts
  }
});
```

### Memory Issues

```typescript
// Monitor memory usage
const memoryMonitor = new MemoryMonitor({
  threshold: 0.8,  // Alert at 80% usage
  interval: 60000  // Check every minute
});

memoryMonitor.on("highUsage", async ({ used, total }) => {
  // Log memory stats
  await logger.warn("High memory usage", {
    used: formatBytes(used),
    total: formatBytes(total),
    percentage: (used / total) * 100
  });
});

// Configure memory limits
const pipeline = new IngestPipeline<Event>("events", {
  stream: {
    maxBufferSize: 1000,    // Limit buffered records
    maxBatchSize: 100       // Limit batch size
  },
  storage: {
    writeBufferSize: "256MB"  // Limit write buffer
  }
});
```

### Data Quality Issues

```typescript
// Monitor data quality
const qualityMonitor = new DataQualityMonitor("metrics", {
  checks: {
    // Check for missing values
    nullRate: {
      fields: ["userId", "timestamp"],
      threshold: 0.01  // Allow 1% nulls
    },
    
    // Check value distributions
    valueDistribution: {
      field: "duration",
      min: 0,
      max: 3600,
      outlierThreshold: 3  // Standard deviations
    },
    
    // Check data freshness
    freshness: {
      field: "timestamp",
      maxDelay: "5m"  // Maximum acceptable delay
    }
  }
});

// Handle quality issues
qualityMonitor.on("qualityIssue", async (issue) => {
  // Log issue details
  await logger.error("Data quality issue detected", {
    check: issue.check,
    field: issue.field,
    value: issue.value,
    threshold: issue.threshold
  });
  
  // Send alert
  await alerting.send({
    title: "Data Quality Issue",
    message: `${issue.check} check failed for ${issue.field}`,
    severity: "high"
  });
});
```

## CLI Commands

DMv2 provides CLI commands for managing pipelines:

```bash
# Check pipeline status
moose pipeline status user_events

# View metrics
moose pipeline metrics user_events --format prometheus

# Replay data
moose pipeline replay user_events --start "2024-03-20" --end "2024-03-21"

# Export data
moose pipeline export user_events --format json --output events.json

# Manage storage
moose storage optimize user_events --compact
moose storage backup user_events --destination s3://backup/

# View logs
moose logs user_events --level error --last 1h
```

## Performance Optimization

### Stream Processing

```typescript
const optimizedPipeline = new IngestPipeline<Event>("events", {
  // Optimize stream processing
  stream: {
    parallelism: 4,              // Parallel processing
    batchSize: 100,             // Larger batches
    bufferSize: 1000,           // Increased buffer
    processingTimeout: 30000,   // Longer timeout
    retryStrategy: {
      maxAttempts: 3,
      backoff: "exponential"
    }
  }
});
```

### Storage Configuration

```typescript
const optimizedStorage = new OlapTable<Record>("records", {
  // Optimize storage
  orderByFields: ["timestamp", "userId"],  // Efficient sorting
  partitionBy: ["toYYYYMM(timestamp)"],   // Time-based partitioning
  
  // Storage settings
  settings: {
    index_granularity: 8192,      // Larger index granularity
    storage_policy: "hot_to_cold",  // Tiered storage
    merge_tree_settings: {
      parts_to_throw_insert: 300,    // More parts before merge
      max_bytes_to_merge: "10GB"     // Larger merge size
    }
  }
});
```

## Next Steps

- Read about [Best Practices](/v2/data-modeling/best-practices)
- Explore [Technical Details](/v2/data-modeling/technical-reference)
- Learn about [Advanced Stream Patterns](/v2/data-modeling/advanced-streams)
- Check [Schema Design](/v2/data-modeling/schemas) 