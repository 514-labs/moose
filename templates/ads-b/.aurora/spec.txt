Here's the updated specification with the new streaming function requirements:

# Building a data intensive feature

## What are you trying to build?
A real-time military aircraft tracking data warehouse that captures and stores ADSB transponder data for historical trend analysis.

### Tell aurora about the application that you're building the data intensive feature into:
This is a data warehousing application that will continuously collect military aircraft transponder data and make it available for real-time access and historical analysis. The system needs to handle potentially dirty/incomplete data and maintain an indefinite historical record.

### Data Intensive Feature Details
* how is the user going to interact with the data intensive feature?
  - Through real-time data access APIs
  - Through historical querying capabilities
  
* what are the filters the user might use?
  - Time-based filters (date ranges)
  - Aircraft-specific identifiers
  - Geographical location
  - Aircraft types/categories
  
* what are the fields the user might sort by?
  - Timestamp of data collection
  - Aircraft identifiers
  - Location coordinates
  - Altitude
  - Speed

## Where is the data that will populate the data intensive feature coming from?

### Pull sources
* API
  ** URL: https://api.adsb.lol/v2/mil
  *** Authentication info: No authentication required
  *** Other info:
      - Pull frequency: Every 5 seconds
      - API documentation: https://api.adsb.lol/docs#/v2/v2_mil_v2_mil_get
      - Data cleanliness: Expect dirty data, fields should be optional where possible
      - Additional requirements: Add collection timestamp if not present in source data

* Database
  - Not applicable

### Batch sources
* File 
  - Not applicable

### Push sources
* Webhook
  - Not applicable

### Streaming Functions
* Location Data Transformation:
  - Input: Latitude and longitude coordinates from source data
  - Output: 
    * Z-order curve coordinate (single dimensional coordinate)
    * Timestamp field
  - Implementation details:
    * Convert 2D geographical coordinates to 1D using Z-order curve algorithm
    * Add timestamp to each transformed record
    * Handle missing or invalid coordinate data gracefully
  - Reference: https://en.wikipedia.org/wiki/Z-order_curve

### Others
* Data Retention Requirements:
  - Retain all historical data indefinitely
  - No data pruning or archival strategy needed

* Performance Requirements:
  - Must support real-time data access
  - Must handle data ingestion every 5 seconds
  - Must support historical trend analysis queries

* Data Quality Considerations:
  - Implementation must be resilient to missing or malformed data
  - System should maintain data lineage by including collection timestamps
  - Should handle API outages or connectivity issues gracefully