# Stream Processing

The Stream component in DMv2 is a versatile data processing unit that can be used independently or as part of a larger data pipeline.

## Core Concepts

### Stream Configuration
```typescript
interface StreamConfig<T> {
  parallelism?: number;          // Number of parallel processors
  retentionPeriod?: number;      // Retention period in seconds
  destination?: OlapTable<T>;    // Optional storage destination
}
```

### Transformation Types
```typescript
// Represents zero, one, or many records
type ZeroOrMany<T> = T | T[] | undefined;

// Transform function type
type SyncOrAsyncTransform<T, U> = (
  record: T
) => ZeroOrMany<U> | Promise<ZeroOrMany<U>>;
```

## Basic Usage

### Creating a Stream
```typescript
import { Stream } from '@514labs/moose-lib';

// Define input/output types
interface RawMetric {
  id: string;
  name: string;
  value: string;
}\

// Create a stream processor
const rawStream = new Stream<RawMetric>("raw_metrics", {
  parallelism: 2,
  retentionPeriod: 86400  // 1 day
});
```

## Integration with Other Components

### 1. Stream to Stream Transformation
```typescript
// Define input type
interface RawMetric {
  id: string;
  name: string;
  value: string;
}

// Define output type
interface ProcessedMetric {
  id: string;
  name: string;
  value: number;
  timestamp: Date;
}

// Create connected streams
const rawStream = new Stream<RawMetric>("raw_metrics");
const processedStream = new Stream<ProcessedMetric>("processed_metrics");

// Add transformation between streams
rawStream.addTransform(processedStream, async (record) => ({
  id: record.id,
  name: record.name,
  value: parseFloat(record.value),
  timestamp: new Date()
}));
```

### 2. Stream with Storage
```typescript
// Create storage table
const metricsTable = new OlapTable<ProcessedMetric>("metrics_storage", {
  orderByFields: ["timestamp", "name"],
  deduplicate: true
});

// Create stream with storage destination
const metricStream = new Stream<ProcessedMetric>("metric_stream", {
  destination: metricsTable
});
```

### 3. Stream with Ingestion
```typescript
// Create stream for processing
const eventStream = new Stream<UserEvent>("user_events");

// Create ingestion API that writes to stream
const eventIngestion = new IngestApi<UserEvent>("events", {
  destination: eventStream
});
```

## Advanced Stream Patterns

### Multi-Stream Routing
```typescript
interface AlertEvent {
  id: string;
  severity: string;
  message: string;
}

// Create destination streams
const processedStream = new Stream<ProcessedMetric>("processed");
const alertStream = new Stream<AlertEvent>("alerts");

// Route records to multiple destinations
rawStream.addMultiTransform((record) => {
  const outputs = [];
  
  // Process metric
  outputs.push(processedStream.routed({
    id: record.id,
    name: record.name,
    value: parseFloat(record.value),
    timestamp: new Date()
  }));
  
  // Generate alert if value too high
  if (parseFloat(record.value) > 100) {
    outputs.push(alertStream.routed({
      id: record.id,
      severity: "high",
      message: `High value detected: ${record.value}`
    }));
  }
  
  return outputs;
});
```

## Using Streams in IngestPipeline

For convenience, DMv2 provides the IngestPipeline component that automatically wires together ingestion, stream processing, and storage. Here's how streams are used within it:

```typescript
const pipeline = new IngestPipeline<UserEvent>("user_events", {
  // Stream is automatically connected to table and ingest
  stream: {
    parallelism: 2,
    retentionPeriod: 86400
  },
  table: true,
  ingest: true
});
```

The IngestPipeline will:
1. Create a Stream instance
2. Connect it to the OlapTable if storage is enabled
3. Connect it to the IngestApi if ingestion is enabled
4. Handle all the wiring automatically
