As always, to get the latest:

```bash
bash -i <(curl -fsSL https://fiveonefour.com/install.sh) moose,aurora
```

## Friday, 16th May 2025

Highlights:
* **Moose and Aurora users** can now embed metadata into their Moose primitives, for use be the users, their agents, and their metadata tools.
* **Aurora users** can read and write from **Databricks** (more coming here soon).
* **Aurora users** can now leverage **DuckDB** for local data wrangling.

### Moose + Aurora

#### Descriptions in your Moose primitives

**Store context about why you are building what you are building, for you and your agents.**

Moose primitives can now include descriptions. 

```ts
const acPipeline = new IngestPipeline<AircraftTrackingProcessed>(
    "AircraftTrackingProcessed",
    {
      table: true,
      stream: true,
      ingest: false,
    },
    { description: "Pipeline for ingesting raw aircraft data" } // new description field!
);
```

**Where is this used?** Aurora tools that create Moose primitives will now write the intention of the Moose primitive into the description field to give tools/agents that work with that primitive in the future more context. Practically, every description is now served to the tools as context too when the infra map is loaded up as context.

**Future of the feature:** Two main extensions of this feature are planned:
* Embedding the descriptions into the underlying infrastructure (e.g. as table level comments in Clickhouse)
* Extending the metadata:
    * To be on a field level as well as a primitive level
    * To include any arbitrary key value pair, not just a description (use this for managing labels like PII!)

#### Quality of life and bug fixes:
* **Flexible JSON ingest**. Our agents were confused by needing to specify whether ingested data was a JSON object or an array of JSON objects. Now by default both work.

### Aurora MCP

#### [Experimental] Wrangler Agent‚ÄîDatabricks tools.

We‚Äôve had a bunch of our users (especially in the enterprise) request deeper integration with Databricks. We‚Äôve created MCP tooling to allow you to read from Databricks, and create new derivative Databricks managed tables.

Turn it on with `aurora config tools`, and adding `experimental-databricks-tools`. [Docs](https://docs.fiveonefour.com/aurora/tool-reference). To connect with your Databricks instance, you‚Äôll need to modify the relevant `MCP.json` file to add:
```JSON
"DATABRICKS_HOST": "[-].databricks.com",
"DATABRICKS_PATH": "/sql/1.0/warehouses/[-]",
"DATABRICKS_TOKEN": "[-]",
```

**This allows you to:‚Ä¢‚Ä¢
* Read from Databricks tables
* Create queries and run them against Databricks tables
* Create new Databricks managed tables

**Future of the feature:**
* **Workflows V2**: We're working on bringing schemas into our workflow creation tools, our Databricks integration will be able to leverage these in interacting with Databricks.
* **DatabricksTable**: We're working on a new primitive that will allow you to create a Databricks table from a Moose primitive.

#### [External] DuckDB MCP

You can now bring DuckDB‚Äôs MCP tool into a Moose Project and manage that MCP through the Aurora CLI\! Try aurora config tools and enable `external-duck-db-mcp`. [Docs](https://docs.fiveonefour.com/aurora/tool-reference). You'll need to configure the MCP with your duckdb path‚Äîeither after selectign the tool-set through `aurora config tools`, or by editing the MCP.json file directly.

String DuckDB tool calls with Aurora tool calls to manipulate local files for ingestion. Try our Aurora tools for creating a DuckDB database from your local flat files.

**Future of the feature:**
* **Remote Data**: We're working on bringing remote data into Moose management with this MCP toolset

#### Data Collection
Aurora is in research preview, to help us improve our product, we‚Äôve added data collection to our MCP tools. Ready-to-go templates default to `comprehensive` data collection (we figure that these templates are least likely to have sensitive information), and standard templates default to `standard` data collection. You can change your data collection preferences whenever you want (just by editing your project‚Äôs `aurora.config.toml` file. [Docs](https://docs.fiveonefour.com/aurora/data-collection-policy).

### Aurora CLI

#### Quality of life and bug fixes:
* **LLM Docs**. Want docs for your LLMs that work with Moose projects? [Here](https://docs.fiveonefour.com/api/llms.txt).
* **Read only tools**. We‚Äôve had requests for a read-only toolset, especially for people using chat based MCP clients. We‚Äôve split standard tools into: read-only and egress-tools. [Docs](https://docs.fiveonefour.com/aurora/tool-reference).
* **Sorting Projects**. Projects in the CLI are now sorted (hopefully y‚Äôall have enough Aurora projects that this is useful)! Affects aurora config tools and structure of ~/.aurora/config.toml. [Docs](https://docs.fiveonefour.com/aurora/tool-reference). Deleted projects will now be cleaned from ~/.aurora/config.toml and from whenever the CLI prints lists of projects.
* **üêõRead tools now only read**. clickhouse read tool was doing a little more than read, especially when used by Claude. Tool has been tightened up, and if you want extra certainty, create a read-only user (see snippet below).

```SQL
CREATE USER username IDENTIFIED BY 'password' SETTINGS PROFILE 'readonly'
GRANT SHOW TABLES, SELECT ON db+name.* TO username
```

### In Progress
Let us know if these features are interesting to you, we are always looking for beta users, design partners, and general feedback\!

* **Google Docs MCP integration**. Get data from Google Docs / Sheets for use as context, or as a source\!  
* **First Party Chat.** Chat with your data from our hosting platform, Boreal.